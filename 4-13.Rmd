---
title: "4.13"
author: "Greg Matesi"
date: "July 5, 2020"
output: pdf_document
---

# Load the data
```{r}
options(digits=4)

# MASS library for the Boston data and lda()
library(MASS)

attach(Boston)

# class library for knn()
library(class)

# Initial look at the data
dim(Boston)
head(Boston)
summary(crim)
```

# Make dummy variable

Make a dummy variable called highcrime. This variable will be "TRUE" if crim is greater than it's median. And it will be "FALSE" if crim is less than it's median.

```{r}
med.crim <- median(crim)
Boston$highcrime <- Boston$crim > med.crim

# remove the crim variable. It has been replaced with highcrime
Boston <- Boston[,-c(1)]
attach(Boston)

names(Boston)
dim(Boston)

# there should be an equal amount of observations above and below the mean
summary(Boston$highcrime)
```

```{r}
# Looking at the highcrime variable.
cor(Boston)
plot(crim)
abline(h=med.crim)
plot(highcrime)
```

# Subset

Subset the data into a training set and a test set. The variable subsetfrac can be adjusted to allow for a larger or smaller fraction to be used as a test set.

```{r}
subsetfrac <- 1/3
train.index <- sample(dim(Boston)[1], (1-subsetfrac)*dim(Boston)[1] )

Boston.test <- Boston[-c(train.index),]
Boston.train <- Boston[c(train.index),]

highcrime.test <- highcrime[-c(train.index)]

```





# Logistic

Fit a logistic regression model using nox, ptratio, chas, and indus as variables.
```{r}
glm.fit <- glm(highcrime~nox+ptratio+chas+indus,
               data = Boston.train,
               family = binomial)

coef(glm.fit)
summary(glm.fit)
```

Predict the probability that a neighborhood will have a high crime rate.
```{r}
glm.probs <- predict(glm.fit,
                     newdata = Boston.test,
                     type = "response")
glm.probs[1:10]
```

Predict whether a neighborhood in the test set will have a high crime rate probability of greater or less than 50%
```{r}
probability <- .5
glm.pred <- rep(FALSE, length(highcrime.test))
glm.pred[glm.probs > probability]=TRUE
```

Assess the model.
```{r}
glm.table <- table(glm.pred,highcrime.test)
glm.table
mean(glm.pred==highcrime.test)
(glm.table[1,1]+glm.table[2,2]) / length(highcrime.test)
```




# LDA

Fit linear discriminate analysis model on the Boston data.
```{r}
lda.fit <- lda(highcrime~nox+ptratio+chas+indus, 
               data=Boston, 
               subset=train.index)
lda.fit
plot(lda.fit)
```

Use the lda model to predict whether neighborhoods in the test set will have a high crime rate.
```{r}
lda.pred=predict(lda.fit, Boston.test)
names(lda.pred)
```

```{r}
lda.class <- lda.pred$class
lda.table <- table(lda.class, highcrime.test)
lda.table
mean(lda.class==highcrime.test)
(lda.table[1,1]+lda.table[2,2]) / length(highcrime.test)

```





# KNN

```{r}
variable.list <- c("nox", "ptratio", "chas", "indus")
train.x <- Boston.train[,variable.list]
test.x  <- Boston.test[,variable.list]
train.highcrime <- cbind(Boston.train$highcrime)
```

```{r}
numk = 1
knn.pred <- knn(train.x, 
                test.x,
                train.highcrime,
                k=numk)

knn.table <- table(knn.pred,highcrime.test)
knn.table

(knn.table[1,1]+knn.table[2,2])/length(highcrime.test)
mean(knn.pred == highcrime.test)
```

# Analysis

We see that lda and logistic regression predicts equally well on the test set. Each of these two models predict correctly for ~85% of the observations. The third model, k nearest neighbors with k=1 outperforms them in this respect. It predicts correctly on the test set ~95% of the time.

## Sensitivity

The true positive rate of these three methods is examined below.

```{r}
glm.table[2,2]/(glm.table[2,2]+glm.table[1,2])
lda.table[2,2]/(lda.table[2,2]+lda.table[1,2])
knn.table[2,2]/(knn.table[2,2]+knn.table[1,2])
```

Knn with k=1 outperforms both of the other two models in sensitivity, the ability to correctly predict a true high crime rate.
## Specificity

The true negative rate for the three models is examined blow.
```{r}
glm.table[1,1]/(glm.table[1,1]+glm.table[2,1])
lda.table[1,1]/(lda.table[1,1]+lda.table[2,1])
knn.table[1,1]/(knn.table[1,1]+knn.table[2,1])
```

We see that knn with k=1 also outperforms the other two models in sensitivity. The ability to correctly predict a low crime rate.
